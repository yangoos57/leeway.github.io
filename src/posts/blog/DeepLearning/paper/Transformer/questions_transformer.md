---
title: "questions about transformer"
category: "DeepLearning"
date: "2022-10-02"
thumbnail: "./img/questions.png"
---

### Attention이 이해가 안된다.

**Attention 의미**
우선 어텐션(attention)은 시퀀스 입력에 수행하는 기계학습 방법의 일종인데요. 어텐션은 시퀀스 요소들 가운데 태스크 수행에 중요한 요소에 집중하고 그렇지 않은 요소는 무시해 태스크 수행 성능을 끌어 올립니다. 어텐션은 기계 번역 과제에 처음 도입됐습니다.

기계 번역에 어텐션을 도입한다면 타깃 언어를 디코딩할 때 소스 언어의 단어 시퀀스 가운데 디코딩에 도움되는 단어들 위주로 취사 선택해서 번역 품질을 끌어 올리게 됩니다. 즉, 어텐션은 디코딩할 때 소스 시퀀스 가운데 중요한 요소들만 추립니다.

셀프 어텐션이란, 말 그대로 자기 자신에 수행하는 어텐션 기법입니다. 입력 시퀀스 가운데 태스크 수행에 의미 있는 요소들 위주로 정보를 추출한다는 것이죠.

**Attention 탄생 배경**
RNN은 시퀀스 길이가 길어질 수록 정보 압축에 문제가 발생합니다. 오래 전에 입력된 단어는 잊어버리거나, 특정 단어 정보를 과도하게 반영해 전체 정보를 왜곡하는 경우가 자주 생긴다는 것이죠. 어텐션은 이러한 문제점을 해결하기 위해 제안됐습니다. 디코더 쪽 RNN에 어텐션을 추가하는 방식입니다. 어텐션은 디코더가 타깃 시퀀스를 생성할 때 소스 시퀀스 전체에서 어떤 요소에 주목해야 할지 알려주므로 카페가 소스 시퀀스 초반에 등장하거나 소스 시퀀스의 길이가 길어지더라도 번역 품질이 떨어지는 것을 막을 수 있습니다. 참고로 그림14의 예시에서는 어텐션 기법으로 주목되는 단어에 좀 더 짙고 굵은 실선을 그려 놓았습니다.

이처럼 개별 단어와 전체 입력 시퀀스를 대상으로 어텐션 계산을 수행해 문맥 전체를 고려하기 때문에 지역적인 문맥만 보는 CNN 대비 강점이 있습니다. 아울러 모든 경우의 수를 고려(단어들 서로가 서로를 1대 1로 바라보게 함)하기 때문에 시퀀스 길이가 길어지더라도 정보를 잊거나 왜곡할 염려가 없습니다. 이는 RNN의 단점을 극복한 지점입니다.

**Multi_head_attention**
멀티-헤드 어텐션(Multi-Head Attention)은 셀프 어텐션(self attention)을 여러 번 수행한 걸 가리킵니다. 여러 헤드가 독자적으로 셀프 어텐션을 계산한다는 이야기입니다. 비유하자면 같은 문서(입력)를 두고 독자(헤드) 여러 명이 함께 읽는 구조라 할 수 있겠습니다.

### Transformer가 훈련하는 과정과 예측하는 과정의 차이?

훈련과 예측에는 decoder 작동 방식이 다름

**훈련**

- 번역으로 예시

- Input(한국어), Output(영어) Set 준비
- 예시 : `어제, 카페, 갔었어, 거기, 사람, 많더라 > I, went, to, the, cafe, There, were, many, people, there`
- Encoder 단에는 한국어 Input에 포함된 단어 간 관계를 정교화 한 임베딩 벡터를 만듬
- 그 결과를 Decoder 구조의 중간 Attention에 연결
- Decoder 단 시작부에는 mask가 쓰여있는 임베딩 벡터를 생성(개념적으로는 순차적으로 학습하는 거지만, 병렬 연산을 위해 모든 단계를 만들어놈)

  <img alt='img16' src='./img/img16.png' style="width : 400px">

- masked attention과 encoder에서 들어오는 정보를 encoder 단 가운데 Attention 파트에서 연산한 뒤 output을 출력

  - 모델 입장(?)에서 봤을 때 input 한국어 문장은 볼 수 있기 때문에, 가장 확률이 높은 영단어 하나를 문장의 첫 시작으로 선택

  - 그 다음은 전체 한국어 문장과 이전에 모델이 선택한 단어들을 종합해서 비교해 다음 단어를 예측함

- 출력한 output의 probability와 실제 output을 비교해 W를 줄임

- **예측 단계와 차이점은 이전 예측했던 단어가 예측값이 아닌 실제 정답으로 바뀐다는 것임.**

  - 가령 모델이 you라고 예측 했어도 다음 예측 할때에는 오답인 you가 정답인 I로 바뀌어 있게 된다.

**예측**

- 훈련 단계에서 설명한 바와 같이, 예측 단계에서는 기계가 예측한 값을 그대로 다음 단어 예측에 활용한다.

- 훈련이라면 모델이 you를 예측했어도 I인 상태에서 다음 단어를 예측했다면 예측에서는 you로 예측하면 you와 한국어 문장 저네를 가지고 다음 단어를 예측한다.

출처 : <a href = 'https://ratsgo.github.io/nlpbook/docs/language_model/transformers/'> Transformer 살펴보기 </a>

### Scaled dot product attention은 무엇을 계산하려는 용도이지?

Scaled dot product attention = attention

단어 별로 관련성 높은 것들을 문장 내에서 매칭시킴
