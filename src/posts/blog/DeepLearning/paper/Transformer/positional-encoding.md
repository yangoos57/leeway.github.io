---
title: "Transformer Positional Encoding 이해하기"
category: "DeepLearning"
date: "2023-03-30"
thumbnail: "./img/transformer.png"
desc: pytorch를 활용해 Transformer 논문을 코드로 구현하며 모델의 상세 작동원리를 설명하였다. 구현한 Transformer 모델을 활용해 학습과 평가하는 과정을 경험할 수 있도록 튜토리얼을 제작했으며, 튜토리얼을 통해 모델 내부에서 어떻게 데이터가 흐르는지, 어떠한 과정을 거쳐 입력 데이터에 대한 결과물을 산출하는지를 이해할 수 있다. 논문에 포함된 Transformer의 도식화 그림을 활용해 Transformer 구조 전반에 대한 이해에 도움을 준다.
---

### 들어가며

이 글은 Transformer의 구조 중 Positional Encoding에 대한 설명과 이에 대한 참고자료를 정리하였습니다.

### 단어의 위치 정보 생성하기

Poisional Encoding은 문장 내 단어의 "위치 정보"를 벡터로 표현한 한 것입니다. 이러한 벡터 값이 왜 필요로 하는지, 어째서 Transformer 모델에 중요한지는 Transformer가 탄생한 배경을 알게 된다면 이해하실 수 있습니다.

기본적으로 Transfoermer의 큰 구조인 Encoder, Decoder는 Transformer 모델에서 새롭게 소개된 구조가 아닙니다. Transformer는 RNN 기반의 seq2seq모델을 Attention을 활용해 구현한 모델일 뿐입니다.

물론 Attention만을 활용해 seq2seq 모델을 구현하는 것 자체가 기술적인 도전이었고, 병렬연산, 문장 내 모든 단어 정보 참조가 가능한 Attention의 장점을 많은 사람들에게 각인 시킨 모델이었기에 Transformer 기반 모델이 사실상 NLP 모델의 표준으로 자리잡게 된 것이라 할 수 있습니다.

따라서 Positional Encoding은 Seq2Seq 구조를 Attention으로 구현하기 위한 기술적 과제 중 하나인 병렬 연산이 가능해짐에 따라 문장 내 단어의 위치 정보를 반영할 수 없는 문제를 해결하기 위한 아이디어로서 바라볼 수 있습니다.

이에 대해 부가적인 설명을 더하자면, RNN 기반의 seq2seq 모델은 문장을 학습할 때 단어(엄밀히 말하면 토큰)하나를 순차적으로 학습 했기에 자동으로 단어 간 간격의 범위에 따라 단어의 의미를 다르게 부여한다거나, 특정 단어가 오는 패턴등을 학습 할 수 있었습니다.

이해를 돕기 위해 예시를 하나 들어보겠습니다. "나는 오늘 학교에 안 갔다"라는 문장이 있습니다. 이 문장을 토크나이징 하게되면 [나, -는, 오늘, 학교, -에, 안, 갔다.]로 구분 됩니다. 여기서 "안"이라는 단어는 부정의 의미로 사용되고 있으며 "안" 다음에 위치하는 토큰인 "갔다"를 통해 부정의 의미를 파악할 수 있습니다. 이 문장과 비슷한 문장인 "나는 오늘 학교 안에 갔다"를 분석해보겠습니다. 여기서의 단어 "안"은 내부를 의미하며 "-에"라는 단어를 통해서 이를 파악할 수 있습니다.

이러한 예시처럼 특정 단어 다음이라는 위치정보 하나만으로도 이 단어가 어떠한 의미로 사용되는지, 어떠한 단어가 나올 가능성이 높은지를 판단할 수 있는 훌륭한 정보로서 활용 될 수 있다는 점을 볼때, 위치 정보를 포함할 수 없는 Attention은 상당한 페널티가 부여 받은 것이라 할 수 있습니다.

그러므로 위치 정보를 반영할 수 없는 문제의 해결방안인 Positional encoding은 Attention 만으로 모델을 구현할 수 있는 핵심이라 할 수 있습니다.

### 중복되지 않은 벡터를 생성하려면

이제 Positional Encoding이 Transformer의 핵심인 이유를 이해했으니 Positional Encoding은 어떠한 방법으로 구현 했으며 이러한 방법을 왜 택했는지를 살펴보겠습니다.

본격적으로 설명하기에 앞서 내용 이해에 필요한 기본 지식인 삼각함수에 대한 유용한 참고자료를 소개하겠습니다. 원리 이해를 기반으로 쉽게 설명한 자료라 sine, cosine에 대한 부담을 가볍게 털어내실 수 있습니다.

- [[깨봉수학] 삼각함수 종합편 초등학생도 10초면 끝! [수학 공부법](Feat. cos)](https://www.youtube.com/watch?v=C_UsgRpyrUM&t=437s)
- [[GongbroDesk]그려보는 수학 | 삼각함수 사인 & 코사인](https://www.youtube.com/watch?v=vT5pQ0-gqJU)

- [[수학방]삼각함수 그래프의 이동, 평행이동, 주기, 최대, 최소](https://mathbang.net/529#gsc.tab=0)

#### ❖ 단어 정보를 해치지 않는 선에서

단어에 위치 정보를 포함하는 방법은 단어 벡터와 위치 정보 벡터를 더하는 방식으로 수행됩니다. 논문에서 단어 하나의 정보를 표현하기 위해 512 차원의 벡터를 활용하므로 단어의 위치 정보 또한 512개 차원의 벡터로 구현됩니다.

이때 위치 정보를 반영할 때 주의할 점이 있습니다. 단어를 표현하는 벡터 의 크기에 영향을 주지 않을 정도로 작은 값이어야 하며, 위치 정보를 표현하는 벡터들은 서로 중복되어서는 안됩니다. 리스트의 인덱스 끼리 겹치는 상황이 있을 수 없는 것과 같습니다.

논문의 저자는 이러한 조건을 만족시키는 단어의 위치 정보 벡터를 생성하기 위해 sin과 cos를 이용합니다. 아래 식은 논문에 소개된 positional encoding 공식이며 이제부터 이 공식을 이해해보도록 하겠습니다.

[공식]

공식에 사용된 pos, i, d는 아래 그림을 통해 바로 알 수 있습니다. 분홍색의 벡터는 pos 번째 위치한 단어의 위치 벡터이며 내부 원소는 i로 표현되고 있습니다. d의 경우는 단어 하나를 표현하는데 사용된 벡터의 차원(논문에서는 512)입니다.

[그림1]
